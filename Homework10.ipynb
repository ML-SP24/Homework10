{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework10\n",
    "\n",
    "Exercises with text processing and NLP modeling\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand similarities and differences between the processes of working with text, images and tabular data\n",
    "- Practice with different methods of encoding and modeling text data\n",
    "- See different methods for extracting information or patterns from text datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/text_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from data_utils import display_silhouette_plots, object_from_json_url\n",
    "from text_utils import get_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell it's gonna be a good homework from the number of imports.\n",
    "# ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have protein, need seasoning\n",
    "\n",
    "Let's create a model to help us season our foods. In the end, what we want is a model that receives a short list of ingredients and returns a list of seasonings or complementary ingredients for our original ingredients list.\n",
    "\n",
    "In order to do that we need a dataset of recipes. We'll load that into a text dataset where each recipe is a document and the ingredients are our document *tokens*.\n",
    "\n",
    "Let's take a look at the recipe dataset and become familiar with the data and how it's organized.\n",
    "\n",
    "We'll load our recipes and do a bit of exploratory data analysis to look for patterns first to see if this kind of modeling makes any sense.\n",
    "\n",
    "### Load Data\n",
    "\n",
    "Here's our dataset. Let's load it into an object for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/refs/heads/main/datasets/text/recipes\"\n",
    "recipes_obj = object_from_json_url(f\"{DATAPATH}/recipes_min16.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Data\n",
    "\n",
    "How's the data organized?\n",
    "\n",
    "How many recipes do we have?\n",
    "\n",
    "Do all recipes have the same number of ingredients?\n",
    "\n",
    "Anything else stand out about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 18009, 'ingredients': ['raisins', 'baking powder', 'egg', 'sugar', 'milk', 'flour']}, {'id': 35687, 'ingredients': ['parmesan cheese', 'salt', 'cornmeal', 'black pepper', 'sausage', 'olive oil', 'leeks', 'water']}, {'id': 38527, 'ingredients': ['salt', 'corn starch', 'butter', 'lemon juice', 'baking powder', 'heavy cream', 'peaches', 'sugar', 'flour']}, {'id': 41217, 'ingredients': ['corn starch', 'orange juice', 'rice', 'ginger', 'vinegar', 'vegetable oil', 'garlic', 'sriracha', 'sesame seeds', 'chicken broth', 'soy sauce', 'egg', 'onion', 'white pepper', 'orange zest', 'sugar']}, {'id': 42969, 'ingredients': ['cilantro', 'rice', 'ginger', 'garlic', 'yogurt', 'curry powder', 'onion', 'cumin']}]\n",
      "5015\n",
      "Shortest recipe has 5 ingredients\n",
      "Longest recipe has 27 ingredients\n"
     ]
    }
   ],
   "source": [
    "# TODO: Look at Data here\n",
    "print(recipes_obj[:5])\n",
    "# TODO: How many recipes\n",
    "print(len(recipes_obj))\n",
    "# TODO: How many ingredients do the shortest and longest recipes have?\n",
    "rec_len = [len(recipe[\"ingredients\"]) for recipe in recipes_obj]\n",
    "shortest = min(rec_len)\n",
    "longest = max(rec_len)\n",
    "print(f\"Shortest recipe has {shortest} ingredients\")\n",
    "print(f\"Longest recipe has {longest} ingredients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Features\n",
    "\n",
    "Our dataset doesn't really have to be a `DataFrame` here. It can, but it doesn't have to be.\n",
    "\n",
    "Each recipe right now is described as a list of ingredients, but what we really want is a list of *sentences*, where each *sentence* is a Python `string` with all of the ingredients for a given recipe.\n",
    "\n",
    "Instead of:<br>```[\"salt\", \"baking soda\", \"water\", \"mushroom\"]```,\n",
    "\n",
    "we want:<br>```\"salt baking soda water mushroom\"```\n",
    "\n",
    "The `join()` function might help.\n",
    "\n",
    "Another thing to consider is wether we want to do anything special about multi-word ingredients, like *baking soda*.\n",
    "\n",
    "Do we want to let our vectorizer (spoiler) split that into two tokens, or do we want to guarantee that *baking* and *soda* always stay together? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parmesan cheese salt cornmeal black pepper sausage olive oil leeks water\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'parmesan cheese salt cornmeal black pepper sausage olive oil leeks water'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: turn list of objects into list of strings\n",
    "for recipe in recipes_obj:\n",
    "   recipe[\"ingredients_list\"] = \" \".join(recipe[\"ingredients\"])\n",
    "\n",
    "\n",
    "print(recipes_obj[1][\"ingredients_list\"])\n",
    "\n",
    "ingredients = [ingredients[\"ingredients_list\"] for ingredients in recipes_obj]\n",
    "ingredients[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Data\n",
    "\n",
    "The fun part.\n",
    "\n",
    "Let's vectorize our list of ingredient strings into a sparse document matrix using `CountVectorizer` or `TfidfVectorizer`.\n",
    "\n",
    "The resulting matrix will have one row for each recipe, and the columns will encode the ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2588"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Vectorize ingredients from our recipe list\n",
    "vec = TfidfVectorizer(stop_words=\"english\", min_df=5, max_df=0.75, max_features=20_000 , ngram_range=(1, 2))\n",
    "recipes_vct = vec.fit_transform(ingredients)\n",
    "\n",
    "# TODO: How many words are in our vocabulary?\n",
    "vocab = vec.get_feature_names_out()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_vct[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Data\n",
    "\n",
    "Now that we have our recipes/documents vectorized we can study them a little bit, and look for patterns.\n",
    "\n",
    "What happens if we cluster our recipes ? What do the cluster centers represent ?\n",
    "\n",
    "When might this be useful ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: cluster recipes\n",
    "cluster_km = KMeans(n_clusters=7, random_state=1010)\n",
    "cluster_km.fit(recipes_vct)\n",
    "ing_kmeans = cluster_km.predict(recipes_vct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "num_clusters = list(range(2,15))\n",
    "\n",
    "# collect distance, silhouette and balance scores\n",
    "score_scores = []\n",
    "\n",
    "# get distance, likelihood and balance for different clustering sizes\n",
    "for n in num_clusters:\n",
    "  mm = KMeans(n_clusters=n)\n",
    "  mm.fit_predict(recipes_vct)\n",
    "  score_scores.append(mm.score(recipes_vct))\n",
    "\n",
    "\n",
    "# plot scores as function of number of clusters\n",
    "plt.plot(num_clusters, score_scores, marker='o')\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Distance Squared Sum Score\")\n",
    "plt.title(\"K-means Clustering\")\n",
    "#plt.ylim([-4300, -3900])\n",
    "#plt.xlim([5, 10])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the number of clusters we need is bigger than 5 and less than 8 or 9. So I will just use k = 7. Afer I saw that two centers had the same word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Centers\n",
    "\n",
    "Use the `get_top_words()` function to decode the `cluster_centers` back into ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Look at cluster centers\n",
    "clusters_centers = cluster_km.cluster_centers_\n",
    "print(clusters_centers)\n",
    "\n",
    "get_top_words(cluster_km.cluster_centers_, vocab, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "What do these cluster centers represent ?<br>\n",
    "Is there anything interesting about recipe cluster centers ?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:lightgreen;\">\n",
    "We are using TF-IDF so it's scaling words on their importance and returns a sparce matrix with importance values. We are passing those values to the clustering K-Means algorithm and creating 7 clusters. The center of each culster is the most important word in each cluster. It's the average value of all points in this cluster. <br>\n",
    "The Interessting thing for me that the clusters are getting ingriedents together that are actually could be cooked together.\n",
    "<br>\n",
    "The interesting thing is also the n-gram range captured ingredients that are fit together like oil and olive oil and still having oil and olive also separated.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Clusters\n",
    "\n",
    "Let's plot our clusters to see if we have to adjust any of the clustering parameters.\n",
    "\n",
    "Since we can't plot in $500$ dimensions, we should use `PCA` to look at our clusters in $2D$ and $3D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: PCA to reduce the dimensions of our feature space\n",
    "pca = PCA(n_components=3)\n",
    "rec_df = pca.fit_transform(recipes_vct.toarray())\n",
    "\n",
    "# TODO: plot clusters \n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.title(\"Clusters of Recipes\")\n",
    "plt.scatter(rec_df[:, 0], rec_df[:, 1], c=ing_kmeans, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Silhouette Plots\n",
    "\n",
    "We can also check the quality of our clustering by looking at the silhouette plots that we get from calling:<br>\n",
    "`display_silhouette_plots(vectors, clusters)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_silhouette_plots(recipes_vct, ing_kmeans)\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "score = silhouette_score(recipes_vct, ing_kmeans)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "How many clusters did you end up with ?<br>\n",
    "How do they look ?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:lightgreen;\">\n",
    "I tried many different. From 3 to 20. <br>\n",
    "\n",
    "The higher clusters the worse. Two clusters is also not useful. \n",
    "The best number was 8 witht the best score and plots. Otherwise the error was bigger across the components. This was after I changed the vectorizer. The Count Vectorizer was not as good. The best number of clusters was 3, which I was not very convinced with. I changed to the TF vetorizer. and it was kind of better with 7 clusterrs which makes more sence as we are talking about food and ingreiedents that could be very diverse. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Completion\n",
    "\n",
    "Ok. On to the main event.\n",
    "\n",
    "Let's create some recipes.\n",
    "\n",
    "We'll do this using a technique similar to what is used for movie/product recommendations. Given an initial set of ingredients, we'll look at recipes that have similar ingredients and \"recommend\" additional ingredients.\n",
    "\n",
    "We already have all of the recipes in our dataset encoded as `tf-idf` vectors. The rest of our algorithm will be something like:\n",
    "1. Start with an initial set of ingredients\n",
    "2. Encode ingredients\n",
    "3. Find a set of recipes that are similar to our list of ingredients\n",
    "4. Find common ingredients that are in the similar recipes, but not in our list of ingredients\n",
    "5. Pick representative ingredient to add to recipe\n",
    "6. Repeat\n",
    "\n",
    "Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial list of ingredients\n",
    "\n",
    "This is just a string with ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_seed_str = \"flour\"  # feel free to change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encode ingredients\n",
    "\n",
    "Transform the string into a `tf-idf` vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: transform string into sparse vector\n",
    "recipe_seed_vct = vec.transform([recipe_seed_str])\n",
    "recipe_seed_vct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find similar recipes\n",
    "\n",
    "The meat of the algorithm. No pun intended.\n",
    "\n",
    "In order to find similar recipes, we'll first calculate the distance between our current list of ingredients and all recipes in our dataset.\n",
    "\n",
    "We can start with euclidean distance and later try other kinds, but the overall processing will be the same:\n",
    "\n",
    "1. Start with an empty list to store distances\n",
    "2. Loop over the `tf-idf` recipe vectors and for each vector:\n",
    "   1. Subtract the ingredient list\n",
    "   2. Square the difference (to square a sparse matrix `A`, use `A.multiply(A)`)\n",
    "   3. Sum the terms of the result\n",
    "   4. Take the square root of the sum\n",
    "   5. Append to distance list\n",
    "3. Find the indices of the smallest distances (this operation is called `argsort` and will give us the indices of the recipes that are most similar to our list of ingredients)\n",
    "4. Check the recipes to see if they are indeed similar (`inverse_transform()` the vectors at the indices calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argsort a list (get sequence of indices that would sort the list)\n",
    "# https://stackoverflow.com/a/3382369\n",
    "def argsort(L, reverse=False):\n",
    "  return sorted(range(len(L)), key=L.__getitem__, reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5015\n",
      "[677, 4405, 69, 1846]\n",
      "salt flour yogurt oil ghee\n",
      "salt cake flour baking soda buttermilk flour\n",
      "salt pistachios butter flour cake flour cherries sugar vanilla\n",
      "salt butter bread flour egg yeast sugar flour\n"
     ]
    }
   ],
   "source": [
    "# TODO: list to keep distances\n",
    "recipe_dists = []\n",
    "\n",
    "# TODO: loop over vectors and append euclidean distances to list\n",
    "for vec in recipes_vct:\n",
    "    diff = recipe_seed_vct - vec\n",
    "    sq_diff = diff.multiply(diff)\n",
    "    sum_sq = sq_diff.sum()\n",
    "    dist = (sum_sq)**0.5\n",
    "    recipe_dists.append(dist)\n",
    "\n",
    "print(len(recipe_dists))\n",
    "\n",
    "# TODO: argsort list of distances to find indices of similar recipes\n",
    "idxs = argsort(recipe_dists)\n",
    "\n",
    "# TODO: check first 4 recipes\n",
    "print(idxs[:4])\n",
    "for i in idxs[:4]:\n",
    "    print(ingredients[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW!! This is amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find ingredients to recommend\n",
    "\n",
    "We have a way to get a set of similar recipes with similar ingredients, and now want to find a *meaningful*, or *representative*, ingredient to add to our ingredients list.\n",
    "\n",
    "Let's consider ingredients in the $16$ most similar recipes. What we are trying to do is find an ingredient that is in a lot of these recipes, but not yet in our list of ingredients.\n",
    "\n",
    "There are many possible ways of doing this. We could count the number of times different ingredients show up in these $16$ recipes using Python dictionaries and/or sets, but what we're trying to do here is very similar to what a `TfidfVectorizer` does: calculate relative importance of terms in a series of documents.\n",
    "\n",
    "Let's re-encode these $16$ recipes using their own separate `TfidfVectorizer`, then sum the importance of each ingredient and look at ingredients with the highest importance scores.\n",
    "\n",
    "We could re-use the vectors/scores from the original `TfidfVectorizer`, but they're gonna be influenced by the relative frequencies of all of the ingredients that showed up in all of the recipes. Using a separate vectorizer is a little bit more precise.\n",
    "\n",
    "The steps we need to take are:\n",
    "\n",
    "1. Separate the $16$ recipes most similar to our list of ingredients\n",
    "   1. We have lots of representations of our recipes, but `recipes` (list of strings) might be the easiest one to use here\n",
    "2. Create a new `TfidfVectorizer` and encode the $16$ recipes\n",
    "3. Sum the resulting vectors to get overall importance scores for each ingredient/token\n",
    "4. Convert resulting vector to a list using `A.tolist()[0]`\n",
    "5. `argsort` the importance scores to get sequence of ingredient indices ordered from most to least important\n",
    "6. Find the most important ingredient that isn't on the ingredient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ['salt flour yogurt oil ghee', 'salt cake flour baking soda buttermilk flour', 'salt pistachios butter flour cake flour cherries sugar vanilla', 'salt butter bread flour egg yeast sugar flour', 'salt wheat flour honey olive oil yeast water flour', 'kimchi salt pork canola oil egg rice flour scallions flour', 'lemon butter egg apples rice flour sugar flour', 'salt butter bread flour egg yeast sugar milk flour', 'wheat flour salt cumin seed flour water ghee', 'wheat flour salt honey black pepper warm water tomato sauce basil corn flour oregano olive oil yeast rosemary flour', 'cream cheese salt butter corn flour egg vanilla sugar milk flour', 'wheat flour salt rice ginger cinnamon peanut oil nutmeg paprika flour', 'butter egg tea sugar flour', 'butter vanilla sugar milk flour', 'salt butter black pepper crab flour', 'salt vegetables egg baking powder shrimp rice flour cold water oil flour']\n",
      "this is ing Score [[5.11735017 4.31274261 5.46509298 4.41377885]]\n",
      "Regular List [5.1173501678099, 4.312742608148488, 5.465092979512857, 4.413778854540591]\n",
      "Indicies [2, 0, 3, 1]\n",
      "['butter' 'egg' 'oil' 'sugar']\n",
      "in the list oil\n",
      "in the list butter\n",
      "in the list sugar\n",
      "not in the list egg\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get 16 most similar recipes\n",
    "sxten_recpies = []\n",
    "for i in idxs[:16]:\n",
    "    sxten_recpies.append(ingredients[i])\n",
    "print(\"First\", sxten_recpies)\n",
    "\n",
    "# TODO: Encode the 16 recipes\n",
    "sxten_vec = TfidfVectorizer(stop_words=\"english\", min_df=5, max_df=0.75, max_features=10_000)\n",
    "sxten_df = sxten_vec.fit_transform(sxten_recpies)\n",
    "\n",
    "# TODO: Sum the recipe vectors by column to get ingredient importance scores\n",
    "ing_score = sxten_df.sum(axis=0)\n",
    "print(\"this is ing Score\",ing_score)\n",
    "\n",
    "# TODO: Convert sparse vector to regular list with A.tolist()[0]\n",
    "vec_list = ing_score.tolist()[0]\n",
    "print(\"Regular List\",vec_list)\n",
    "\n",
    "# TODO: argsort the importance scores\n",
    "sxten_im = argsort(vec_list, reverse=True) #True for the most important (before we wanted the least not the most)\n",
    "print(\"Indicies\", sxten_im)\n",
    "\n",
    "# TODO: Find most important ingredient not yet on the list of ingredients\n",
    "vocab = sxten_vec.get_feature_names_out()\n",
    "print(vocab)\n",
    "\n",
    "for idx in sxten_im:\n",
    "    ingredient = vocab[idx]\n",
    "    if ingredient not in recipe_seed_str:\n",
    "        print(\"not in the list\", ingredient)\n",
    "    else: \n",
    "        print(\"in the list\", ingredient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add ingredient to recipe\n",
    "\n",
    "This is simply adding a word to `recipe_seed_str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: add the first important ingredient to list of ingredients\n",
    "for idx in sxten_im[:1]:\n",
    "    ingredient = vocab[idx]\n",
    "    if ingredient not in recipe_seed_str:\n",
    "        recipe_seed_str = f'{recipe_seed_str} {ingredient}'\n",
    "recipe_seed_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Repeat (Optional)\n",
    "\n",
    "Now we can repeat this process until we get an empty list of important ingredients: \n",
    "1. Encode current recipe\n",
    "2. Find similar recipes\n",
    "3. Find important ingredients\n",
    "4. Add important ingredient\n",
    "\n",
    "Might be helpful to define a couple of functions, like `find_similar_recipes()` and `find_important_ingredients()`...\n",
    "\n",
    "Only do this step if you're really curious about experimenting with generating unconventional ingredient lists. It's not going to be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create find_similar_recipes(ingredients, recipes, vectorizer)\n",
    "def find_similar_recipes(recipes, seed, vectorizer):\n",
    "    ##v Vectorizing\n",
    "    recipes_vct = vectorizer.fit_transform(recipes)\n",
    "    seed_vec = vectorizer.transform([seed])\n",
    "    \n",
    "    ## Distance from the seed to each vector\n",
    "    distance = []\n",
    "    for vector in recipes_vct:\n",
    "        difference = seed_vec - vector\n",
    "        difference_squared = difference.multiply(difference)\n",
    "        squared_sum = difference_squared.sum()\n",
    "        sum_square_root = (squared_sum)**0.5\n",
    "        distance.append(sum_square_root)\n",
    "    \n",
    "    #sorts the nearst to the seed vector\n",
    "    indicis = argsort(distance)\n",
    "\n",
    "    top_ten_recipes = []\n",
    "    for index in indicis[:15]:\n",
    "        recipe = recipes[index]\n",
    "        top_ten_recipes.append(recipe)\n",
    "\n",
    "\n",
    "    return top_ten_recipes\n",
    "\n",
    "# TODO: Create find_important_ingredients(recipes)\n",
    "def find_important_ingredients(recipes):\n",
    "    ## Encoding\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=5, max_df=0.75, max_features=30_000, ngram_range=(1,2))\n",
    "    recipes_vec = vectorizer.fit_transform(recipes)\n",
    "\n",
    "    ## Score for each ingredient in the sparse matrix\n",
    "    ingredients_score = recipes_vec.sum(axis=0)\n",
    "\n",
    "    ## Sparse score vector to regular list\n",
    "    regular_list = ingredients_score.tolist()[0]\n",
    "\n",
    "    ## Sort the most important ingredients top to buttom\n",
    "    indicis = argsort(regular_list, reverse=True)\n",
    "\n",
    "    ## The actual words\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    top_ingredients = []\n",
    "    for index in indicis[:10]:\n",
    "        ingredient = vocab[index]\n",
    "        top_ingredients.append(ingredient)\n",
    "\n",
    "    return top_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recipes = ingredients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=5, max_df=0.75, max_features=30_000 , ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"flour\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "butter was added to your recipe\n",
      "sugar was added to your recipe\n",
      "sugar flour was added to your recipe\n",
      "salt was added to your recipe\n",
      "egg was added to your recipe\n",
      "salt egg was added to your recipe\n",
      "vanilla was added to your recipe\n",
      "egg vanilla was added to your recipe\n",
      "milk was added to your recipe\n",
      "sugar milk was added to your recipe\n",
      "vanilla sugar was added to your recipe\n",
      "milk flour was added to your recipe\n",
      "salt butter was added to your recipe\n",
      "No new ingredients found\n",
      "MY FINAL RECIPE: flour butter sugar sugar flour salt egg salt egg vanilla egg vanilla milk sugar milk vanilla sugar milk flour salt butter\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create recipe by repeating calls to find_similar_recipes() and find_important_ingredients()\n",
    "seed_set = set(seed.split())\n",
    "\n",
    "for rount in range(25):\n",
    "    recipes = find_similar_recipes(all_recipes, seed, vectorizer) ## 10 related recipes\n",
    "    top_ingredient = find_important_ingredients(recipes) ## 5 Top related ingredients\n",
    "\n",
    "    added = False \n",
    "    for ingredient in top_ingredient:\n",
    "        if ingredient not in seed_set:    \n",
    "            seed += f' {ingredient}'\n",
    "            seed_set.add(ingredient)\n",
    "            print(f'{ingredient} was added to your recipe')\n",
    "            added = True\n",
    "            break\n",
    "    if not added: \n",
    "        print(\"No new ingredients found\")\n",
    "        break\n",
    "\n",
    "print(f'MY FINAL RECIPE: {seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, we will have very sweet sugar vanilla cake maybe with a bit of salt!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
